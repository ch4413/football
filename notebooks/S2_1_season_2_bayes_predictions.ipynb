{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "from scipy.stats import gamma\n",
    "\n",
    "import footballanalysis.transform.transform as ft\n",
    "import footballanalysis.model.bayes as fmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from results.csv\n",
    "results = pd.read_csv('../data/results.csv')\n",
    "teams = pd.read_csv('../data/teams.csv')\n",
    "fixtures = pd.read_csv('../data/fixtures.csv')\n",
    "# Define the outcome variable\n",
    "results['Outcome'] = results.apply(\n",
    "    lambda row: 'HomeWin' if row['HomeScore'] > row['AwayScore'] \n",
    "    else ('Draw' if row['HomeScore'] == row['AwayScore'] else 'AwayWin'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = results[['Gameweek', 'HomeTeamID', 'HomeScore']]\n",
    "home.columns = ['Gameweek', 'TeamID', 'Score']\n",
    "away = results[['Gameweek', 'AwayTeamID', 'AwayScore']]\n",
    "away.columns = ['Gameweek', 'TeamID', 'Score']\n",
    "goals_df = pd.concat([home, away])\n",
    "\n",
    "teamids = sorted(goals_df['TeamID'].drop_duplicates().to_list())\n",
    "past_goals = {str(x): goals_df[goals_df['TeamID']==x]['Score'].to_list() for x in teamids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_20764\\4224335821.py:13: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(500)\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu_28, mu_27, mu_26, mu_25, mu_24, mu_23, mu_22, mu_21, mu_20, mu_19, mu_18, mu_17, mu_16, mu_15, mu_14, mu_13, mu_12, mu_11, mu_10, mu_9, mu_8, mu_7, mu_6, mu_5, mu_4, mu_3, mu_2, mu_1, beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6000' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6000/6000 00:24<00:00 Sampling 4 chains, 13 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 500 draw iterations (4_000 + 2_000 draws total) took 52 seconds.\n",
      "There were 7 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 2 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 4 divergences after tuning. Increase `target_accept` or reparameterize.\n"
     ]
    }
   ],
   "source": [
    "model = pm.Model()\n",
    "\n",
    "with model:\n",
    "    alpha = pm.Exponential('alpha', lam=1)\n",
    "    beta = pm.Exponential('beta', lam=1)\n",
    "    \n",
    "    mu = dict()\n",
    "    goals = dict()\n",
    "    for name, observed in past_goals.items():\n",
    "        mu[name] = pm.Gamma('mu_'+ str(name), alpha, beta)\n",
    "        goals[name] = pm.Poisson(name, mu[name], observed=observed)\n",
    "        \n",
    "    trace = pm.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10000/10000 01:45<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with model:\n",
    "    post_pred = pm.sample_posterior_predictive(trace, samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of results\n",
    "results_df = pd.DataFrame({x : post_pred[x].flatten() for x in post_pred.keys()})\n",
    "# Get HomeID, Away ID\n",
    "homeid = fixtures['HomeTeamID'].to_list()\n",
    "awayid = fixtures['AwayTeamID'].to_list()\n",
    "gameweek = fixtures['Gameweek'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results from all seasons, filter for season 2\n",
    "season2 = results[results['SeasonID']==2]\n",
    "results_df_m = pd.DataFrame([fmb.predict_match(str(homeid[i]),\n",
    "                                           str(awayid[i]),\n",
    "                                           gameweek[i],\n",
    "                                           results_df) for i in range(len(homeid))])\n",
    "results_df_m['HomeTeamID'] = results_df_m['HomeTeamID'].astype('int')\n",
    "results_df_m['AwayTeamID'] = results_df_m['AwayTeamID'].astype('int')\n",
    "\n",
    "results_df_m['Prob_outcome'] = results_df_m.apply(lambda row: fmb.prob_result(row['HomeWin'], row['AwayWin'],\n",
    "                                                             row['Draw']), axis=1)\n",
    "season2_outcome = season2['Outcome'].reset_index()\n",
    "results_df_m['Outcome'] = season2_outcome['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6084656084656085"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results_df_m['Prob_outcome']==results_df_m['Outcome']).sum() / results_df_m.shape[0]\n",
    "# to add: precision, recall, f1 score, confusion matrix, Cohen's kappa?, log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict season outcome x1000 - done, nice work\n",
    "homeids = fixtures['HomeTeamID'].to_list()\n",
    "awayids = fixtures['AwayTeamID'].to_list()\n",
    "sim_results = np.zeros([756, 2, 1000])\n",
    "for i in range(fixtures.shape[0]):\n",
    "    homeid = str(homeids[i])\n",
    "    awayid = str(awayids[i])\n",
    "    homescore = results_df[homeid].sample(1000).values\n",
    "    awayscore = results_df[awayid].sample(1000).values\n",
    "    sim_results[i, 0, :] = homescore\n",
    "    sim_results[i, 1, :] = awayscore\n",
    "    \n",
    "table_finishes = {x : [] for x in fixtures['HomeTeamID'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_finishes = {x : [] for x in fixtures['HomeTeamID'].unique()}\n",
    "for x in range(1000):\n",
    "    points_by_week_sim = fmb.create_sim_season(fixtures, sim_results, x)\n",
    "    final_table_sim = (points_by_week_sim.groupby(['ID']).agg({'Points': 'sum', 'For': 'sum', 'Against': 'sum'})\n",
    "                .reset_index()\n",
    "                .sort_values('Points', ascending=False)\n",
    "                )\n",
    "    final_table_sim = final_table_sim.reset_index(drop=True)\n",
    "    [table_finishes[final_table_sim['ID'][i]].append(i+1) for i in range(final_table_sim.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('table_finishes.pkl', 'wb') as file:\n",
    "    pickle.dump(table_finishes, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
